{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409ac14a-da70-42ed-a7f8-40bd59a8e0fd",
   "metadata": {},
   "source": [
    "# Differentiating Policies for Non-Myopic Rollout Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66a8c5-83e2-4294-b057-d8d592a39aee",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mux}{\\mu(x)}\n",
    "\\newcommand{\\sigx}{\\sigma(x)}\n",
    "\\newcommand{\\calpha}{\\check{\\alpha}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\r",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "$$\n",
    "Throughout these notes, we consider the derivatives of a user-provided function needed for Newton's method and for differentiation of the argmax with respect to data and hyperparameters. We have some collection of regressors $X \\in \\mathbb{R}^{d \\times n}$ and observations $\\mathcal{\\textbf{y}} \\in \\mathbb{R}^{n}$, where our dataset for our supervised machine learning model is denoted as $\\mathcal{D}_n = \\{(x^i, y_i) : 1 \\leq i \\leq n\\}$. Given $\\mathcal{D}_{n}$, we denote the predictive mean and predictive variance at some location $x$ as $\\mu(x|\\mathcal{D}_n)$ and $\\sigma(x|\\mathcal{D}_n)$ respectively. In general, we'll suppress the parameter $\\mathcal{D}_n$, and write $\\mu_n(x)$ and $\\sigma_n(x)$.\n",
    "\n",
    "Now, we consider a user-provided function in terms of the predictive mean, predictive variance, and hyperparameters. We also consider the case where the cost to evaluate is non-uniform, i.e. $c(x): \\mathbb{R}^d \\to \\mathbb{R}$, giving us the general base policy\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\alpha(x, \\theta) &= g(\\mu_n(x), \\sigma_n(x), \\theta).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba719e-a0bd-4fc0-bc5d-06c199deeff3",
   "metadata": {},
   "source": [
    "We define an $h$-step rollout policy in terms of the user-provided function $f$ as choosing $x_0, \\theta_0$ based on the anticipated behavior of $\\alpha$ starting from $x_0, \\theta_0$ and proceeding for $h$ steps. That is, we consider the iteration\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "x_r, \\theta_r = \\argmax_{x \\in \\chi, \\theta \\in \\Theta}\\; \\alpha(x, \\theta), \\; 1 \\leq r \\leq h\n",
    "$$\n",
    "where each iteration defines a trajectory consisting of $h+1$ decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18840662-77b7-42ee-8005-f3bb59b45039",
   "metadata": {},
   "source": [
    "We'll need derivative of $\\check{\\alpha}$ w.r.t. $x$ and $\\theta$, i.e.:\n",
    "$$\\begin{aligned}\n",
    "\\alpha_{,i} &= \\frac{\\partial g}{\\partial \\mu}\\mu_{,i} + \\frac{\\partial g}{\\partial \\sigma} \\sigma_{,i}\\\\\n",
    "\\alpha_{,ij} &= \\frac{\\partial^2 g}{\\partial\\mu^2}\\mu_{,i} + \\frac{\\partial g}{\\partial\\mu}\\mu_{,ij} + \\frac{\\partial^2 g}{\\partial\\sigma^2}\\sigma_{,i} + \\frac{\\partial g}{\\partial\\sigma}\\sigma_{,ij}\\\\\n",
    "\\dot{\\alpha}_{,i} &= \n",
    "    \\frac{\\partial g}{\\partial \\mu \\partial \\mathcal{D}_n}\\mu_{,i} + \\frac{\\partial g}{\\partial \\mu}\\dot{\\mu}_{,i} + \\frac{\\partial g}{\\partial \\sigma \\partial \\mathcal{D}_n} \\sigma_{,i} + \\frac{\\partial g}{\\partial \\sigma} \\dot{\\sigma}_{,i}\\\\\n",
    "\\frac{\\partial \\alpha}{\\partial \\theta} &= \\frac{\\partial g}{\\partial \\theta} \\\\\n",
    "\\frac{\\partial^2 \\alpha}{\\partial \\theta^2} &= \\frac{\\partial^2 g}{\\partial \\theta^2} \\\\\n",
    "\\frac{\\partial \\alpha}{\\partial x \\partial\\theta} &= \\frac{\\partial^2 g}{\\partial \\theta^2} \\\\\n",
    "\\frac{\\partial \\alpha}{\\partial \\theta \\partial \\mathcal{D}_n} &= \\frac{\\partial g}{\\partial \\theta \\partial \\mathcal{D}_n}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e746d-1feb-481f-aa89-5ed9c43cfebf",
   "metadata": {},
   "source": [
    "## Rollout and Adjoint Mode Differentiation\n",
    "Consider a rollout trajectory to time step $t$.  We are interested in computing the derivative of the function value $y_t$ at time $t$  respect to the initial condition $\\bfx_0$.\r",
    "\r",
    "The basic setup for the forward computation is\r",
    "\\begin{align*}\r",
    "  \\bfr_j(\\bfx_j; \\bfx_0, y_0, \\ldots, \\bfx_{j-1}, y_{j-1}, \\theta) &= 0 \\\\\r",
    "  f(\\bfx_j) - y_j &= 0.\r",
    "\\end{align*}\r",
    "Here $\\bfr_j$ should be thought of as the gradient of the acquisition function at step $j$ (i.e. we seek to maximize the acquisition function at each step).  I will not write out all the details of this calculation here, as we have it elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c68eaf-7246-4808-b9b0-f181bd78814a",
   "metadata": {},
   "source": [
    "We can write the usual \"forward mode\" computation of the derivatives by differentiating each equation in term and then doing forward substitution.\r\n",
    "$$\r",
    "\\begin{bmatrix}\r",
    "  & f'(\\bfx_0) & -1 \\\\\r",
    "  \\frac{\\partial \\bfr_1}{\\partial \\theta} &\n",
    "  \\frac{\\partial \\bfr_1}{\\partial \\bfx_0} &\r",
    "  \\frac{\\partial \\bfr_1}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_1}{\\partial \\bfx_1} \\\\\r",
    "    &   & f'(\\bfx_1) & -1 \\\\\r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\theta} &\n",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_0} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_1} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial y_1} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_2} \\\\\r",
    "  &   &   &   & f'(\\bfx_2) & -1 \\\\\r",
    "  \\vdots & & & & & & \\ddots \\\\\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\theta} &\n",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_0} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_1} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial y_1} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_2} &\r",
    "  \\cdots &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_{t-1}} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_t} \\\\\r",
    "  &   &   &   & & & & f'(\\bfx_t) & -1 \r",
    "\\end{bmatrix}\r",
    "\\begin{bmatrix}\n",
    "  \\delta \\theta \\\\\r",
    "  \\delta \\bfx_0 \\\\ \\delta y_0 \\\\\r",
    "  \\delta \\bfx_1 \\\\ \\delta y_1 \\\\\r",
    "  \\delta \\bfx_2 \\\\ \\delta y_2 \\\\\r",
    "  \\vdots \\\\\r",
    "  \\delta \\bfx_t \\\\ \\delta y_t\r",
    "\\end{bmatrix} = 0.\r",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ff4ea-3a77-4678-a713-4dd284a3ac0d",
   "metadata": {},
   "source": [
    "We are interested in sensitivity with respect to $\\delta \\bfx_0$ and $\\delta \\theta$, so we rewrite this as\r",
    "$$\r",
    "\\begin{bmatrix}\r",
    "  -1 \\\\\r",
    "  \\frac{\\partial \\bfr_1}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_1}{\\partial \\bfx_1} \\\\\r",
    "    & f'(\\bfx_1) & -1 \\\\\r",
    "  \\frac{\\partial \\bfr_2}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_1} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial y_1} &\r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_2} \\\\\r",
    "   &   &   & f'(\\bfx_2) & -1 \\\\\r",
    "  \\vdots & & & & & \\ddots \\\\\r",
    "  \\frac{\\partial \\bfr_t}{\\partial y_0} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_1} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial y_1} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_2} &\r",
    "  \\cdots &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_{t-1}} &\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_t} \\\\\r",
    "   &   &   & & & & f'(\\bfx_t) & -1 \r",
    "\\end{bmatrix}\r",
    "\\begin{bmatrix}\r",
    "  \\delta y_0 \\\\\r",
    "  \\delta \\bfx_1 \\\\ \\delta y_1 \\\\\r",
    "  \\delta \\bfx_2 \\\\ \\delta y_2 \\\\\r",
    "  \\vdots \\\\\r",
    "  \\delta \\bfx_t \\\\ \\delta y_t\r",
    "\\end{bmatrix} = \n",
    "-\\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  \\frac{\\partial \\bfr_1}{\\partial \\theta} \\\\\n",
    "  0 \\\\ \n",
    "  \\frac{\\partial \\bfr_2}{\\partial \\theta} \\\\\n",
    "  0 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial \\bfr_t}{\\partial \\theta} \\\\\n",
    "  0\n",
    "\\end{bmatrix} \\delta \\theta\r",
    "-\\begin{bmatrix}\r",
    "  f'(\\bfx_0) \\\\\r",
    "  \\frac{\\partial \\bfr_1}{\\partial \\bfx_0} \\\\\r",
    "  0 \\\\ \r",
    "  \\frac{\\partial \\bfr_2}{\\partial \\bfx_0} \\\\\r",
    "  0 \\\\\r",
    "  \\vdots \\\\\r",
    "  \\frac{\\partial \\bfr_t}{\\partial \\bfx_0} \\\\\r",
    "  0\r",
    "\\end{bmatrix} \\delta \\bfx_0.\r",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a31329-1582-48ef-9fde-5a46b776fe4f",
   "metadata": {},
   "source": [
    "We can write this more concisely as $L v = -q \\, \\delta \\theta -g \\, \\delta \\bfx_0,$ where $L$ is a (block) lower triangular matrix, $v$ is the vector of variations, and $-q \\, \\delta \\theta -g \\, \\delta \\bfx_0$ is the right hand side. We are interested in derivatives with respect to $x_0$ and $\\theta$, i.e.:\n",
    "$$\\begin{aligned}\n",
    "\\delta y_t = -e_m L^{-1} g \\delta x_0 \\;\\land\\;\n",
    "\\delta y_t = -e_m L^{-1} q \\delta \\theta\n",
    "\\end{aligned}$$\n",
    "where $e_m$ is the last column of the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf4e47-1362-423a-b935-ed76bcc17d58",
   "metadata": {},
   "source": [
    "## Inquiry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6e683-8e24-48d0-86d0-b3c7b49ad830",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "$$\n",
    "\n",
    "The above assumes the same hyperparameter per solve of our base policy. Here, we want to consider the case where each solve is with respect to a unique hyperparameter, i.e.\n",
    "$$\n",
    "\\bfr_j(\\bfx_j, \\theta_j; \\bfx_0, y_0, \\ldots, \\bfx_{j-1}, y_{j-1}) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2841e-0f86-42f9-92ac-7f4d08d6fa85",
   "metadata": {},
   "source": [
    "## Recommendations from David\n",
    "Make a table of notations. Use macros when typesetting for convenience. So things match what I write in code. Appendix and main document to stay consistent via macros.\n",
    "\n",
    "New projects?\n",
    "* Variants on Knowledge Gradient\n",
    "* Optimal Selection of Exploration Hyperparameters\n",
    "    * Learning to Learn paper. Rollout on $\\theta$\n",
    "* Cost-Constrained Non-Myopic Bayesian Optimization\n",
    "    * Stochastic Model for Cost-Function \n",
    "* Cloud Resource Management\n",
    "* Sampling Different GPs with Different Model Hyperparameters\n",
    "    * Cheap to do with respect to what we have now with rollout.\n",
    "\n",
    "Universal Kriging. RNN for Scheduling Policy as a function of function characteristics. Maybe train a neural net on the rollout data and function characteristics. Some set of hypothesized function classes, not that it is a particular function. Now let me look at data and see which function class my current thing belongs to. Bayesian Information Criterion for model selection. Akaike Information Criterion. Minimum Description Length.\n",
    "\n",
    "Adaptive experimentation. Assume a distribution over function families.\n",
    "\n",
    "David and I are kicking around some things. Would love to chat about them. Is there a good time? 3 way if the schedules intersect.\n",
    "\n",
    "Play around with something that is less Monte-Carlo. Maybe Laplace Approximation around the mean values of the rollout. This will give us something about the uncertainty (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430c2b35-266c-437f-955a-01160f6d5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e9fa7c-d461-49f0-a044-08208254ee62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "centered_fd (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function centered_fd(f, u, du, h)\n",
    "    (f(u+h*du)-f(u-h*du))/(2h)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "998a7443-8253-4978-888e-3d2669e6e788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mixed_partials (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = exp(x[1]) * x[2]^2\n",
    "g(x) = ForwardDiff.gradient(f, x)\n",
    "mixed_partials(x) = ForwardDiff.jacobian(g, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2771721-ce88-4a24-8190-e9fd4bdae881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.553753741782685"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1., 1.]\n",
    "h = 1e-6\n",
    "δx = rand(2)\n",
    "\n",
    "gx = dot(g(x), δx)\n",
    "gx_fd = centered_fd(f, x, δx, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58cfd24b-4bc9-4f40-b6f6-c85a9f2b9c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.62475504626509e-11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(gx - gx_fd) / (norm(gx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c764796f-55d0-41ad-8fa9-5b2cac1e7b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 2.71828  5.43656\n",
       " 5.43656  5.43656"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_partials(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c7de143-442d-47d6-b1ca-7b27de97dfb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `outer` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `outer` not defined",
      ""
     ]
    }
   ],
   "source": [
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b10941-f64a-47a9-a7cc-5617d265cf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia nodeps 1.10.5",
   "language": "julia",
   "name": "julia-nodeps-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
